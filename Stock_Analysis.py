import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
import matplotlib.pyplot as plt


# Load the dataset
file_path = '/content/005930.KS.csv'  # Make sure the file path is correct in your environment
data = pd.read_csv(file_path, index_col='Date', parse_dates=True)

# Display the first few rows of the dataset
print(data.head())

# Check for any missing values
print(data.isnull().sum())

# Fill any missing values (if any) with the previous value
data.fillna(method='ffill', inplace=True)


# Feature Engineering: Create technical indicators
data['SMA_20'] = data['Close'].rolling(window=20).mean()
data['EMA_12'] = data['Close'].ewm(span=12, adjust=False).mean()

# Drop rows with NaN values generated by moving averages
data.dropna(inplace=True)

# Display the updated dataset with new features
print(data[['Close', 'SMA_20', 'EMA_12']].tail())



# Scale the data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(data[['Close', 'SMA_20', 'EMA_12']])

# Prepare the training dataset
time_step = 60
X, y = [], []
for i in range(time_step, len(scaled_data)):
    X.append(scaled_data[i-time_step:i])
    y.append(scaled_data[i, 0])

X, y = np.array(X), np.array(y)

# Split into training and testing datasets
train_size = int(len(X) * 0.8)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]





# Build the LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, batch_size=64, epochs=50)


# Make predictions on the test data
predictions = model.predict(X_test)

# Rescale the predictions back to the original scale
predictions_rescaled = scaler.inverse_transform(np.concatenate((predictions, np.zeros((predictions.shape[0], 2))), axis=1))[:,0]
y_test_rescaled = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], 2))), axis=1))[:,0]

# Calculate accuracy metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)
mse = mean_squared_error(y_test_rescaled, predictions_rescaled)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_rescaled, predictions_rescaled)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")




# Function to predict future stock prices
def predict_future_prices(model, scaled_data, time_step, n_future):
    future_predictions = []
    
    # Starting with the last time_step from the training data
    current_input = scaled_data[-time_step:]
    
    for _ in range(n_future):
        current_input_reshaped = current_input.reshape(1, time_step, scaled_data.shape[1])
        prediction = model.predict(current_input_reshaped, verbose=0)
        future_predictions.append(prediction[0, 0])
        
        # Update current_input with the new prediction
        next_input = np.zeros_like(current_input[-1])  # Create a new input array with the same shape
        next_input[0] = prediction[0, 0]  # Assign the predicted value to the first feature
        current_input = np.concatenate((current_input[1:], next_input.reshape(1, -1)), axis=0)
        
    return future_predictions

# Define the number of future days to predict
n_future = 80

# Predict future stock prices
future_predictions = predict_future_prices(model, scaled_data, time_step, n_future)

# Rescale the future predictions back to the original scale
future_predictions_rescaled = scaler.inverse_transform(
    np.concatenate((np.array(future_predictions).reshape(-1, 1), 
                    np.zeros((n_future, scaled_data.shape[1] - 1))), 
                   axis=1))[:, 0]

# Plot the future predictions
plt.figure(figsize=(14, 7))
plt.plot(data.index[-len(y_test_rescaled):], y_test_rescaled, color='blue', label='Actual Stock Price')
plt.plot(pd.date_range(start=data.index[-1], periods=n_future, freq='B'), future_predictions_rescaled, 
         color='green', linestyle='dashed', label='Predicted Future Stock Price')

plt.title('Actual vs Predicted Future Stock Prices')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.grid(True)
plt.show()




# Plotting the actual vs predicted stock prices for the test set
plt.figure(figsize=(14, 7))

plt.plot(data.index[-len(y_test_rescaled):], y_test_rescaled, color='blue', label='Actual Stock Price')
plt.plot(data.index[-len(predictions_rescaled):], predictions_rescaled, color='red', linestyle='dashed', label='Predicted Stock Price')

plt.title('Actual vs Predicted Stock Prices')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.grid(True)
plt.show()

# Print accuracy metrics
print("Model Performance on Test Data:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R²): {r2:.4f}")

# Additional interpretation
if r2 > 0.7:
    print("The model has a good fit, capturing most of the variability in the data.")
elif r2 > 0.5:
    print("The model has a moderate fit, capturing a reasonable amount of variability.")
else:
    print("The model's fit is poor, suggesting that it may not be capturing the underlying trends effectively.")
